{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **$\\color{red}{\\text{Ensemble Learning}}$**\n",
    "### In general, is a model that makes predictions based on a number of different models. By combining individual models, the ensemble model tends to be more flexible (less bias) and les data-sensitive (less variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **$\\color{red}{\\text{Two Most Popular ensemble methods:}}$**\n",
    "\n",
    "### 1. Bagging - Training a bunch of individual models in a parallel way. Each model is trained by a random subset of the data.\n",
    "### 2. Boosting - Training a bunch of individual models in a sequential way. Each individual model learns from mistakes made by the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load library\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets\n",
    "X, y = make_moons(n_samples = 10000, noise = 0.5, random_state = 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit a decision tree as comparision - no ensemble\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **$\\color{red}{\\text{Random Forest}}$**\n",
    "### an ensemble model using bagging as the ensemble method and decision tree as the individual model. Each individual decision tree model uses a random subset of features to split the tree. \n",
    "\n",
    "### Step 1. Split the training dataset into n subsets.\n",
    "### Step 2. Train n individual decision tree models in parallel. The optimal splits for each decision tree are based on a random subset of features. \n",
    "### Step 3. Each decision tree generates a prediction, independently. \n",
    "### Step 4. Voting - For each candidate in the test set, Random Forest uses the class with the majority vote as this candidate's final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, \n",
    "                            max_features = \"auto\",\n",
    "                            random_state = 58)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.79\n",
      "Run Time =  0.9505100250244141 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100, \n",
    "                            max_features = \"auto\",\n",
    "                            random_state = 58)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Run Time = \", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **$\\color{red}{\\text{AdaBoost (Adaptive Boosting)}}$**\n",
    "\n",
    "### a boosting ensemble model and works especially well with the decision tree. Boosting model's key is learning from the previous mistakes. \n",
    "\n",
    "### Step 1. Initialize the weight for datapoints.\n",
    "### Step 2. Train a decision tree using entire train set.\n",
    "### Step 3. Calculate the weighted error rate (e, measure of how many wrong predictions out of total and you treat the wrong)\n",
    "### Step 4. Re-calculate decision tree's weights in the ensemble.\n",
    "            Weight = LearningRate * log((1-e)/e)\n",
    "            --> if the weighted error rate is high, lower decision power the tree will be given during voting\n",
    "            --> if the weighted error rate is low, higher decision power the tree will be given during voting\n",
    "### Step 5. Update weights \n",
    "            --> if the model gets prediction correctly, the weight stays the same;\n",
    "            --> if the model gets prediction wrong, the updated weight = old_weight * np.exp(weight)\n",
    "### Step 6. Repeat from step 2, until all models are trained\n",
    "### Step 7. Final prediction - adding up the weight (of each tree) multiply the prediction (of each tree) based on power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(n_estimators = 100)\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.8235\n",
      "Run Time =  0.5093522071838379 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators = 100)\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Run Time = \", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **$\\color{red}{\\text{Gradient Boosting}}$**\n",
    "### It learns from previous model's mistake - residual errors directly, ranther than update the weights of datapoints.\n",
    "            residual error = actual y - predicted \n",
    "### Step 1. Train a decision tree on entire training set\n",
    "### Step 2. Apply the decision tree to get prediction results\n",
    "### Step 3. Calculate residual errors (save residual errors as the new y)\n",
    "### Step 4. Repeat from step 1.\n",
    "### Step 5. Final prediction - make a new prediction by simply adding up the predictions of all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators = 100)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.82\n",
      "Run Time =  0.3449513912200928 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators = 100)\n",
    "gb.fit(X_train, y_train)\n",
    "y_pred = gb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Run Time = \", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link: https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
